import numpy as np
import torch


def Q610_to_float(hex_list):
    result = []
    for hex_val in hex_list:
        val = int(hex_val, 16)
        if val >= 0x8000:
            val -= 0x10000
        real_val = val / 1024.0
        result.append((real_val))
    return result


def baseline_softmax_FP32(x):
    logits_tensor = np.array(x)
    max_elem = logits_tensor.max()
    logits_tensor = logits_tensor - max_elem
    prob_softmax = np.exp(logits_tensor) / sum(np.exp(logits_tensor))
    return prob_softmax


def split_by_n(s, n=4):
    return [s[i : i + n] for i in range(0, len(s), n)]


softmax_in = "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"


softmax_in_arr = split_by_n(softmax_in)

softmax_out = "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"


softmax_out_arr = split_by_n(softmax_out)

in_converted = Q610_to_float(softmax_in_arr)
in_converted = np.array(in_converted).reshape(16, 64)
out_converted = Q610_to_float(softmax_out_arr)
out_converted = np.array(out_converted).reshape(16, 64)


for i in range(16):
    out_ideal = baseline_softmax_FP32(in_converted[i])
    for j in range(64):
        print(f"{in_converted[i][j]:.5f} {out_converted[i][j]:.5f} {out_ideal[j]:.5f}")
    print()

with open("softmax_compare.txt", "w") as txtfile:
    for i in range(16):
        out_ideal = baseline_softmax_FP32(in_converted[i])
        for j in range(64):
            txtfile.write(
                f"{in_converted[i][j]:.5f}  {out_converted[i][j]:.5f}  {out_ideal[j]:.5f}\n"
            )
        txtfile.write("\n")
